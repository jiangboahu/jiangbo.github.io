<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Xiao Wang</title>
</head>
<body>
<div id="layout-content">
<div id="toptitle">
<h1>Xiao Wang (王逍)</h1>
</div>
<table class="imgtable"><tr><td>
<a href="https://wangxiao5791509.github.io/"><img src="picture/xiaowang.JPG" alt="alt text" width="140px" /></a>&nbsp;</td>
<td align="left"><p>Associate Professor<br />
School of Computer Science and Technology <br />
Anhui University <br />
Hefei City, Anhui Province, China <br />
Email: wangxiaocvpr@foxmail.com; xiaowang@ahu.edu.cn; xiaowangahu@ieee.org <br />
<br />
<a href="https://scholar.google.com/citations?hl=en&view_op=list_works&gmla=AJsN-F5y9bxxP-FRWEf7aiRuT51F8TV9OiT1uk7E-Ak0IMOTvmHoW6B5gJwLsBJ6tkj1XunyQWh2rhbfQLf-cUo6gZb6mgFO33k9D_qfPqCTyivEjOmXZuM&user=oq9awGMAAAAJ">[Google Scholar]</a> 
<a href="https://github.com/wangxiao5791509">[GitHub]</a> 
<a href="https://www.zhihu.com/people/wangxiaocvpr">[Zhihu]</a>
<a href="https://dblp.uni-trier.de/pid/49/67-14.html">[DBLP]</a>
<a href="https://orcid.org/0000-0001-6117-6745">[ORCID]</a>
<a href="https://www.cnblogs.com/wangxiaocvpr/">[Blog]</a>
<a href="https://weibo.com/5070353058">[Weibo]</a>
<a href="https://www.youtube.com/channel/UCSmlgXQRqFsUI--mQm90JwQ">[YouTube]</a>
<a href="https://github.com/Event-AHU">[Event-AHU]</a> 
</td>

</tr></table>

<h2>Biography</h2>
<p>I received the B.S. degree and the Ph.D. degree from West Anhui University (Luan, China, in 2013) and <a href="http://cs.ahu.edu.cn/">[<b>Anhui University</b>]</a> 
  (Hefei, China, in 2019). In my Ph.D. career, I was supervised by Professor <b>Jin Tang</b> and <b>Bin Luo</b> 
  (Lab: <a href="https://mmic.lolimay.cn/home">[<b>Multi-Modal Intelligent Computing Group</b>]</a>). 
  From 2015 and 2016, I was a visiting student at the School of Data and Computer Science, <b>Sun Yat-sen University</b>, Guangzhou, China, 
  and supervised by Professor <a href="http://www.linliang.net/">[<b>Liang Lin</b>]</a>. 
  From 7 July to 20 November 2019, I was studying in the J17 Sydney Data Science Hub, <b>University of Sydney</b>, 
  and supervised by Professor <a href="https://www.sydney.edu.au/engineering/about/our-people/academic-staff/dacheng-tao.html">[<b>Dacheng Tao</b>]</a>. 
  From 9 April 2020 to 8 April 2022, I begin my Postdoc career at <a href="https://www.pcl.ac.cn/">[<b>Pengcheng Laboratory</b>]</a> (Shenzhen, China) 
  and cooperate with Professor 
  <a href="https://eeis.ustc.edu.cn/2014/0423/c2648a20109/page.htm">[<b>Feng Wu</b>]</a>, 
  <a href="https://www.pkuml.org/staff/yhtian.html">[<b>Yonghong Tian</b>]</a>, and 
  <a href="https://scholar.google.com/citations?user=o_DllmIAAAAJ&hl=zh-CN&oi=ao">[<b>Yaowei Wang</b>]</a>. 
  Currently, I work at the <a href="http://cs.ahu.edu.cn/">[School of Computer Science and Technology]</a> of <b>Anhui University</b>, 
  Hefei, China, as an Associate Professor. 

<p>My current research interests are mainly in vision-based artificial intelligence and pattern recognition.  
  I am a reviewer for IEEE TCSVT, TIP, IJCV, CVIU, PR, CVPR, ICCV, AAAI, ECCV, ICLR, ACCV, ACM-MM, and WACV. </p>

  
  
<h2>Research Topic</h2>
<ul>
<li><p><b>Object Tracking</b>: Single-, Multi-Object Tracking  
<li><p><b>Multi-modal</b>: Vision-Language, Vision-Thermal (Infrared), Vision-Depth, Vision-Event, Vision-Audio  
<li><p><b>Neuromorphic Vision</b>: Detection and Tracking, Action Recognition, Scene Reconstruction
</ul>
  

  
  
  
  
  
  
<h2>Selected Publications</h2> 
  
  


<ul> 
<li><p><a href="">Few-Shot Learning Meets Transformer: Unified Query-Support Transformers for Few-Shot Classification</a> <br />
  Xixi Wang, <b>Xiao Wang</b>, Bo Jiang, Bin Luo <br />
<i>in peer review, 2022 
  <a href="https://arxiv.org/abs/2208.12398">[arXiv]</a>
</i></p>
</li>
</ul>


  
<ul> 
<li><p><a href="">Learning Spatial-Frequency Transformer for Visual Object Tracking</a> <br />
  Chuanming Tang, <b>Xiao Wang*</b>, Yuanchao Bai, Zhe Wu, Jianlin Zhang, Yongmei Huang <br />
<i>in peer review, 2022 
  <a href="https://arxiv.org/abs/2208.08829">[arXiv]</a>
  <a href="https://github.com/Tchuanm/SFTransT">[Github]</a> 
</i></p>
</li>
</ul>

  

<ul> 
<li><p><a href="">Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey</a> <br />
 <b>Xiao Wang</b>, Guangyao Chen, Guangwu Qian, Pengcheng Gao, Xiao-Yong Wei, Yaowei Wang, Yonghong Tian, Wen Gao <br />
<i>in peer review, 2022 
  <a href="">[arXiv]</a>
  <a href="https://github.com/wangxiao5791509/MultiModal_BigModels_Survey">[Github]</a> 
</i></p>
</li>
</ul>
  

<ul>
<li><p><a href="">See Finer, See More: Implicit Modality Alignment for Text-based Person Retrieval</a> <br />
Xiujun Shu, Wei Wen, Haoqian Wu, Keyu Chen, Yiran Song, Ruizhi Qiao, Bo Ren, <b>Xiao Wang*</b> <br />
<i>The 2nd Workshop on Real-World Surveillance: Applications and Challenges, ECCVW-2022
  <a href="https://arxiv.org/abs/2208.08608">[arXiv]</a>
  <a href="https://vap.aau.dk/rws-eccv2022/">[ECCV]</a>
  <a href="https://github.com/shuxjweb/IVT">[Code]</a>
</i></p>
</li>
</ul>
  
  





<ul>
<li><p><a href="https://arxiv.org/abs/2207.12767">Criteria Comparative Learning for Real-scene Image Super-Resolution</a> <br />
Yukai Shi, Hao Li, Sen Zhang, Zhijing Yang, <b>Xiao Wang</b> <br />
<i>IEEE Transactions on Circuits and Systems for Video Technology, 2022 
  <a href="https://arxiv.org/abs/2207.12767">[arXiv]</a>
  <a href="https://ieeexplore.ieee.org/document/9847265">[IEEE]</a>
  <a href="https://github.com/House-Leo/RealSR-CCL">[Code]</a> 
  <a href="https://github.com/House-Leo/RealSR-Zero">[Dataset]</a> 
</i></p>
</li>
</ul>


<ul>
<li><p><a href="https://arxiv.org/abs/2205.13125">Prompt-based Learning for Unpaired Image Captioning</a> <br />
Peipei Zhu, <b>Xiao Wang</b>, Lin Zhu, Zhenglong Sun, Weishi Zheng, Yaowei Wang, Changwen Chen <br />
<i>arXiv preprint, 2022, arXiv:2205.13125. 
  <a href="https://arxiv.org/abs/2205.13125">[arXiv]</a>
</i></p>
</li>
</ul>


<ul>
<li><p><a href="https://arxiv.org/abs/2107.10433">MFGNet: Dynamic Modality-Aware Filter Generation for RGB-T Tracking</a> <br />
<b>Xiao Wang</b>, Xiujun Shu, Shiliang Zhang, Bo Jiang, Yaowei Wang, Yonghong Tian, Feng Wu <br />
<i>IEEE Transactions on Multimedia (TMM), 2022. 
  <a href="https://arxiv.org/abs/2107.10433">[arXiv]</a>
  <a href="https://ieeexplore.ieee.org/document/9772993">[IEEE]</a>
  <a href="https://github.com/wangxiao5791509/MFG_RGBT_Tracking_PyTorch">[Code]</a>
</i></p>
</li>
</ul>


<ul>
<li><p><a href="https://arxiv.org/abs/2203.03195">Unpaired Image Captioning by Image-level Weakly-Supervised Visual Concept Recognition</a> <br />
Peipei Zhu, <b>Xiao Wang</b>, Yong Luo, Zhenglong Sun, Wei-Shi Zheng, Yaowei Wang, Changwen Chen <br />
<i>IEEE Transactions on Multimedia (TMM), 2022. <a href="https://arxiv.org/abs/2203.03195">[arXiv]</a>
</i></p>
</li>
</ul>
  

<ul>
<li><p><a href="https://arxiv.org/abs/2202.05659">Tiny Object Tracking: A Large-scale Dataset and A Baseline</a> <br />
Yabin Zhu, Chenglong Li, Yao Liu, <b>Xiao Wang</b>, Jin Tang, Bin Luo, Zhixiang Huang <br />
<i>in peer review, 2022. <a href="https://arxiv.org/abs/2202.05659">[arXiv]</a>
  <a href="https://github.com/ZYB0726/MKDNet">[Code]</a>
</i></p>
</li>
</ul>


  
<ul>
<li><p><a href="https://arxiv.org/pdf/2201.10943.pdf">Event-based Video Reconstruction via Potential-assisted Spiking Neural Network</a> <br />
Lin Zhu, <b>Xiao Wang</b>, Yi Chang, Jianing Li, Tiejun Huang, Yonghong Tian <br />
<i>CVPR, 2022. <a href="https://arxiv.org/pdf/2201.10943.pdf">[arXiv]</a>
  <a href="https://github.com/LinZhu111/EVSNN">[Code]</a>
</i></p>
</li>
</ul>

  
<ul>
<li><p><a href="https://arxiv.org/abs/2112.01177">MutualFormer: Multi-Modality Representation Learning via Mutual Transformer</a> <br />
Wang Xixi, Jiang Bo, <b>Xiao Wang</b>, Luo Bin <br />
<i>in peer review, 2021. <a href="https://arxiv.org/abs/2112.01177">[arXiv]</a>
</i></p>
</li>
</ul>

  
<ul>
<li><p><a href="https://www.aaai.org/AAAI22Papers/AAAI-1396.LiJ.pdf">Retinomorphic Object Detection in Asynchronous Visual Streams</a> <br />
Jianing Li+, <b>Xiao Wang+</b>, Lin Zhu, Jia Li, Tiejun Huang, Yonghong Tian (+ denotes equal contribution) <br />
<i>AAAI, 2022, <font color="#FF0000">Oral representation</font>. 
<a href="https://www.aaai.org/AAAI22Papers/AAAI-1396.LiJ.pdf">[PDF]</a>
<a href="https://aaai-2022.virtualchair.net/poster_aaai1396">[Poster]</a>
<a href="https://www.pkuml.org/resources/pku-vidar-dvs.html">[Project Page]</a>
<a href="https://git.openi.org.cn/lijianing/PKU-Vidar-DVS/datasets">[Dataset]</a>
</i></p>
</li>
</ul>
  
  
  
   
  
<ul>
<li><p><a href="https://arxiv.org/abs/2108.05015">VisEvent: Reliable Object Tracking via Collaboration of Frame and Event Flows</a> <br />
<b>Xiao Wang</b>, Jianing Li, Lin Zhu, Zhipeng Zhang, Zhe Chen, Xin Li, Yaowei Wang, Yonghong Tian, Feng Wu <br />
<i>in peer review, 2021. 
<a href="https://arxiv.org/abs/2108.05015">[arXiv]</a>
<a href="https://sites.google.com/view/viseventtrack/">[Project]</a>
<a href="https://www.youtube.com/watch?v=U4uUjci9Gjc&ab_channel=XiaoWang">[Demo]</a>
<a href="https://www.youtube.com/watch?v=vGwHI2d2AX0&ab_channel=XiaoWang">[Video Tutorial]</a>
<a href="https://github.com/wangxiao5791509/VisEvent_SOT_Benchmark">[Dataset and Evaluation Toolkit]</a>
<a href="https://github.com/wangxiao5791509/RGB-DVS-SOT-Baselines">[Source Code]</a>
</i></p>
</li>
</ul>
  
  
  
<ul>
<li><p><a href="https://arxiv.org/abs/2205.09676">Beyond Greedy Search: Tracking by Multi-Agent Reinforcement Learning-based Beam Search</a> <br />
<b>Xiao Wang</b>, Zhe Chen, Bo Jiang, Jin Tang, Bin Luo, Dacheng Tao <br />
<i>IEEE Transactions on Image Processing (TIP), 2022. 
<a href="https://arxiv.org/abs/2205.09676">[arXiv]</a>
<a href="https://ieeexplore.ieee.org/document/9904473">[IEEE]</a>
<a href="https://sites.google.com/view/beamtracking/">[Project]</a>
<a href="https://www.youtube.com/watch?v=f1yiYv-SJyY&ab_channel=XiaoWang">[Video]</a>
</i></p>
</li>
</ul>
  
  
  
<ul>
<li><p><a href="https://arxiv.org/abs/2205.09676">RGBT Tracking via Cross-modality Message Passing</a> <br />
Rui Yang, <b>Xiao Wang</b>, Chenglong Li, Jinmin Hu, Jin Tang <br />
<i>Neurocomputing, 2021. 
<a href="https://www.sciencedirect.com/science/article/pii/S0925231221011966">[PDF]</a>
</i></p>
</li>
</ul>
  

<ul>
<li><p><a href="https://arxiv.org/abs/2108.00803">Learn to Match: Automatic Matching Network Design for Visual Tracking</a> <br />
Zhipeng Zhang, Yihao Liu, <b>Xiao Wang</b>, Bing Li, Weiming Hu <br />
<i>ICCV, 2021. 
<a href="https://arxiv.org/abs/2108.00803">[arXiv]</a>
<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_Learn_To_Match_Automatic_Matching_Network_Design_for_Visual_Tracking_ICCV_2021_paper.pdf">[ICCV]</a>
<a href="https://github.com/JudasDie/SOTS">[Code]</a>
<a href="https://www.youtube.com/watch?v=iABDcmi3gVI&ab_channel=XiaoWang">[Video]</a>
<a href="https://drive.google.com/file/d/1CcBuGyivdSmvp8dqtrt4kttQEsrkQETG/view">[Poster]</a>
</i></p>
</li>
</ul>


<ul>
<li><p><a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhu_NeuSpike-Net_High_Speed_Video_Reconstruction_via_Bio-Inspired_Neuromorphic_Cameras_ICCV_2021_paper.pdf">NeuSpike-Net: High Speed Image Reconstruction via Bio-inspired Neuromorphic Cameras</a> <br />
Lin Zhu, Jianing Li, <b>Xiao Wang</b>, Tiejun Huang, Yonghong Tian <br />
<i>ICCV, 2021. 
<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhu_NeuSpike-Net_High_Speed_Video_Reconstruction_via_Bio-Inspired_Neuromorphic_Cameras_ICCV_2021_paper.pdf">[PDF]</a>
<a href="https://sites.google.com/view/retina-recon">[Project]</a>
</i></p>
</li>
</ul>
  
  
<ul>
<li><p><a href="https://ieeexplore.ieee.org/document/9463711">Semantic-guided Pixel Sampling for Cloth-Changing Person Re-identification</a> <br />
Xiujun Shu, Ge Li, <b>Xiao Wang</b>, Weijian Ruan, Qi Tian <br />
<i>IEEE Signal Processing Letters, 2021. 
<a href="https://ieeexplore.ieee.org/document/9463711">[IEEE]</a>
<a href="https://arxiv.org/pdf/2107.11522.pdf">[arXiv]</a>
<a href="https://github.com/shuxjweb/pixel_sampling">[Code]</a>
</i></p>
</li>
</ul> 

  
<ul>
<li><p><a href="https://ieeexplore.ieee.org/abstract/document/9615085">Large-Scale Spatio-Temporal Person Re-identification: Algorithms and Benchmark</a> <br />
Xiujun Shu+, <b>Xiao Wang+</b>, Shiliang Zhang, Xianghao Zhang, Yuanqi Chen, Ge Li, Qi Tian (+ denotes equal contribution) <br />
<i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2021. 
<a href="https://ieeexplore.ieee.org/abstract/document/9615085">[IEEE]</a>
<a href="https://arxiv.org/pdf/2105.15076.pdf">[arXiv]</a>
<a href="https://sites.google.com/view/personreid">[Project]</a>
<a href="https://github.com/shuxjweb/last">[Code]</a>
</i></p>
</li>
</ul>
  
  
  
<ul>
<li><p><a href="https://arxiv.org/abs/2103.16746">Towards More Flexible and Accurate Object Tracking with Natural Language: Algorithms and Benchmark</a> <br />
<b>Xiao Wang+</b>, Xiujun Shu+, Zhipeng Zhang, Bo Jiang, Yaowei Wang, Yonghong Tian, Feng Wu (+ denotes equal contribution) <br />
<i>CVPR, 2021. 
<a href="https://arxiv.org/abs/2103.16746">[arXiv]</a>
<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Towards_More_Flexible_and_Accurate_Object_Tracking_With_Natural_Language_CVPR_2021_paper.pdf">[CVF]</a>
<a href="https://sites.google.com/view/langtrackbenchmark/">[Project]</a>
<a href="https://github.com/wangxiao5791509/TNL2K_evaluation_toolkit">[EvaluationToolkit]</a>
<a href="https://www.youtube.com/watch?v=7lvVDlkkff0&ab_channel=XiaoWang">[Demo]</a>
<a href="https://www.youtube.com/watch?v=uE2qnNf-ClI&ab_channel=XiaoWang">[Tutorial]</a>
</i></p>
</li>
</ul>
  


<ul>
<li><p><a href="https://arxiv.org/abs/2106.04840">Tracking by Joint Local and Global Search: A Target-aware Attention based Approach</a> <br />
<b>Xiao Wang</b>, Jin Tang, Bin Luo, Yaowei Wang, Yonghong Tian, and Feng Wu <br />
<i>IEEE Transactions on Neural Networks and Learning Systems (TNNLS), 2021. 
<a href="https://arxiv.org/abs/2106.04840">[arXiv]</a>
<a href="https://ieeexplore.ieee.org/document/9511641">[IEEE]</a>
<a href="https://github.com/wangxiao5791509/LGSearch_DDGAN_PyTorch">[Code]</a>
<a href="https://www.youtube.com/watch?v=uu00QIL7tjo&ab_channel=XiaoWang">[Tutorial]</a>
</i></p>
</li>
</ul>
  
  

<ul>
<li><p><a href="https://arxiv.org/pdf/2007.03584.pdf">STADB: A Self-Thresholding Attention Guided ADB Network for Person Re-identification</a> <br />
Bo Jiang, Sheng Wang, <b>Xiao Wang</b>*, Aihua Zheng (* denotes corresponding author) <br />
<i>arxiv preprint arXiv:2007.03584, 2021. 
<a href="https://arxiv.org/pdf/2007.03584.pdf">[arXiv]</a>
<a href="https://github.com/wangxiao5791509/STADB_ReID">[Code]</a>
</i></p>
</li>
</ul>
  
  
<ul>
<li><p><a href="https://arxiv.org/pdf/1912.10280.pdf">cmSalGAN: RGB-D Salient Object Detection with Cross-View Generative Adversarial Networks</a> <br />
Bo Jiang, Zitai Zhou, <b>Xiao Wang*</b>, Jin Tang, Bin Luo (* denotes corresponding author) <br />
<i>IEEE Transactions on Multimedia (TMM), 2020. 
<a href="https://arxiv.org/pdf/1912.10280.pdf">[arXiv]</a>
<a href="https://ieeexplore.ieee.org/abstract/document/9103135">[IEEE]</a>
<a href="https://github.com/wangxiao5791509/cmSalGAN_PyTorch">[Code]</a>
<a href="https://sites.google.com/view/cmsalgan/">[Project]</a>
</i></p>
</li>
</ul>
  
  
<ul>
<li><p><a href="https://ieeexplore.ieee.org/abstract/document/9345930/">Dynamic Attention-guided Multi-Trajectory Analysis for Single Object Tracking</a> <br />
<b>Xiao Wang</b>, Zhe Chen, Jin Tang, Bin Luo, Yaowei Wang, Yonghong Tian, Feng Wu <br />
<i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2021. 
<a href="https://arxiv.org/abs/2103.16086">[arXiv]</a>
<a href="https://ieeexplore.ieee.org/abstract/document/9345930/">[IEEE]</a>
<a href="https://github.com/wangxiao5791509/DeepMTA_PyTorch">[Code]</a>
<a href="https://sites.google.com/view/mt-track/home/">[Project]</a>
</i></p>
</li>
</ul>
  
  
<ul>
<li><p><a href="https://arxiv.org/abs/1811.10014">Describe and Attend to Track: Learning Natural Language guided Structural Representation and Visual Attention for Object Tracking</a> <br />
<b>Xiao Wang</b>, Chenglong Li, Rui Yang, Tianzhu Zhang, Jin Tang, Bin Luo <br />
<i>arxiv, preprint, arXiv:1811.10014, 2018. 
<a href="https://arxiv.org/abs/1811.10014">[arXiv]</a>
<a href="https://sites.google.com/view/languagetracking/">[Project]</a>
</i></p>
</li>
</ul>

  
<ul>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S0923596518308683">Quality–aware dual–modal saliency detection via deep reinforcement learning</a> <br />
<b>Xiao Wang</b>, Tao Sun, Rui Yang, Chenglong Li, Bin Luo, Jin Tang <br />
<i>Signal Processing and Image Communication, 2019. 
<a href="https://www.sciencedirect.com/science/article/pii/S0923596518308683">[PDF]</a>
</i></p>
</li>
</ul>

  
<ul>
<li><p><a href="https://bmvc2019.org/wp-content/uploads/papers/0562-paper.pdf">Learning Target-aware Attention for Robust Tracking with Conditional Adversarial Network</a> <br />
<b>Xiao Wang</b>, Tao Sun,  Rui Yang, Bin Luo  <br />
<i>30TH British Machine Vision Conference (BMVC), 2019. 
<a href="https://bmvc2019.org/wp-content/uploads/papers/0562-paper.pdf">[PDF]</a>
<a href="https://bmvc2019.org/wp-content/uploads/papers/0562-supplementary.pdf">[Supplementary]</a>
<a href="https://sites.google.com/view/globalattentiontracking">[Project]</a>
</i></p>
</li>
</ul>
  
  
  
<ul>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S0031320321004015">Pedestrian Attribute Recognition: A Survey</a> <br />
<b>Xiao Wang</b>, Shaofei Zheng, Rui Yang, Aihua Zheng, Zhe Chen, Bin Luo, Jin Tang  <br />
<i>Pattern Recognition, 2021. 
<a href="https://arxiv.org/abs/1901.07474">[arXiv]</a>
<a href="https://www.sciencedirect.com/science/article/pii/S0031320321004015">[Pattern Recognition]</a>
<a href="https://sites.google.com/view/ahu-pedestrianattributes/">[Project]</a>
<a href="https://github.com/wangxiao5791509/Pedestrian-Attribute-Recognition-Paper-List">[Github]</a>
</i></p>
</li>
</ul>
  

<ul>
<li><p><a href="https://arxiv.org/pdf/1908.04441.pdf">Learning Target-oriented Dual Attention for Robust RGB-T Tracking</a> <br />
Rui Yang, Yabin Zhu, <b>Xiao Wang</b>, Chenglong Li, Jin Tang  <br />
<i>IEEE International Conference on Image Processing (ICIP), 2019. 
<a href="https://arxiv.org/pdf/1908.04441.pdf">[arXiv]</a>
</i></p>
</li>
</ul>

  

<ul>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S092523121931255X">Multi-modal foreground detection via inter- and intra-modality-consistent low-rank separation</a> <br />
Aihua Zheng,  Naipeng Ye, Chenglong Li, <b>Xiao Wang</b>, Jin Tang <br />
<i>Neurocomputing, 2020. 
<a href="https://www.sciencedirect.com/science/article/pii/S092523121931255X">[PDF]</a>
</i></p>
</li>
</ul>
  
  
  
<ul>
<li><p><a href="Dense Feature Aggregation and Pruning for RGBT Tracking">Dense Feature Aggregation and Pruning for RGBT Tracking</a> <br />
Yabin Zhu, Chenglong Li, Bin Luo, Jin Tang, <b>Xiao Wang</b>  <br />
<i>Proceedings of the 27th ACM International Conference on Multimedia (ACM MM), 2019
<a href="https://arxiv.org/abs/1907.10451">[arXiv]</a>
<a href="https://dl.acm.org/doi/abs/10.1145/3343031.3350928?casa_token=X-2D-Vf5XNIAAAAA:FP95TAXyz0gMYafxBEwbLCxG-fVXNFN8MC2TUPx1BxQsiFD4CgArbxObOQcSa-WA4MIAeWHAe8cKktk">[ACM MM]</a>
</i></p>
</li>
</ul>


<ul>
<li><p><a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_SINT_Robust_Visual_CVPR_2018_paper.pdf">SINT++: Robust Visual Tracking via Adversarial Positive Instance Generation</a> <br />
<b>Xiao Wang</b>, Chenglong Li, Bin Luo, Jin Tang <br />
<i>CVPR, 2018
<a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_SINT_Robust_Visual_CVPR_2018_paper.pdf">[PDF]</a>
<a href="https://sites.google.com/view/cvpr2018sintplusplus/">[Project]</a>
</i></p>
</li>
</ul>
  


<ul>
<li><p><a href="https://ieeexplore.ieee.org/document/7457366">Weighted Low-Rank Decomposition for Robust Grayscale-Thermal Foreground Detection</a> <br />
Chenglong Li+, <b>Xiao Wang+</b>, Lei Zhang, Jin Tang, Hejun Wu, Liang Lin (+ denotes equal contribution) <br />
<i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2017
<a href="https://ieeexplore.ieee.org/document/7457366">[PDF]</a>
<a href="https://sites.google.com/view/mmmovingobjectdetectiontip/">[Project]</a>
</i></p>
</li>
</ul>
  
  
  
<ul>
<li><p><a href="https://ieeexplore.ieee.org/abstract/document/7937922">Deep Co-Space: Sample Mining Across Feature Transformation for Semi-Supervised Learning</a> <br />
Ziliang Chen, Keze Wang, <b>Xiao Wang</b>, Pai Peng, Ebroul Izquierdo, Liang Lin  <br />
<i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2017
<a href="https://ieeexplore.ieee.org/abstract/document/7937922">[PDF]</a>
</i></p>
</li>
</ul>
  
  
  
<ul>
<li><p><a href="https://ieeexplore.ieee.org/abstract/document/7822984/">Grayscale-Thermal Object Tracking via Multitask Laplacian Sparse Representation</a> <br />
Li, Chenglong and Sun, Xiang and <b>Xiao Wang</b> and Zhang, Lei and Tang, Jin <br />
<i>IEEE Transactions on Systems, Man, and Cybernetics: Systems, 2017
<a href="https://ieeexplore.ieee.org/abstract/document/7822984/">[PDF]</a>
</i></p>
</li>
</ul>
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
<h2>Class Schedule</h2>
<ul>
<li><p>2022-2023    MATLAB 编程基础 <br />
<li><p>2022-2023    数据挖掘与知识发现 <br />
</ul>

  
  
  
  
  
  

  
  
  

<h2>Awards</h2>
<ul>
<li><p>2014-2015    National Academic Scholarships <br />
<li><p>2015-2016    National Academic Scholarships <br />
<li><p>2016-2017    National Academic Scholarships <br />
<li><p>2017-2018    National Academic Scholarships <br />
<li><p>2018-2019    National Academic Scholarships <br />
<li><p>2018-2019    GuoYuan Scholarships <br />
<li><p>2019		AHU-University graduates <br />
<li><p>2019		AnHui Outstanding Graduates <br />
</ul>

  
  
<h2>Academic Activities</h2>
<ul>  
<li><p>2022.05.22    IEEE Member (Member number: 96976176)
<li><p>2022.05.30    CSIG Member (Member number: E654403548M)
</ul>
  
  
  

<h2>Funds and Projects</h2>
<ul>
<li><p>Postdoctoral Innovative Talent Support Program (BX20200174, 2021-2022) <br />
<li><p>China Postdoctoral Science Foundation Funded Project (2020M682828, 2021-2022) <br />
<li><p>National Natural Science Foundation of China (62102205, 2022-2024) <br />

  
  
  
<br />
</ul>
<div id="footer">
<div id="footer-text">
<br>Page generated 2022-09-10, by <a href="https://wangxiao5791509.github.io/">Xiao Wang, School of Computer Science and Technology, Anhui University (安徽大学-计算机科学与技术学院-王逍)</a>.
</div>
</div>
</div>
</body>
</html>

  
  
  
  

  
  
  
  
  
  
